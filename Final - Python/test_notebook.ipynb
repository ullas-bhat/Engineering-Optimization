{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries for optimization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite difference methods:\n",
    "\n",
    "# Forward difference:\n",
    "def forward_diff(f, x, h=1E-08):\n",
    "    # f: function\n",
    "    # x: vector of variables\n",
    "    # h: step size\n",
    "\n",
    "    delta_f = np.zeros(x.shape) # Initialize delta_f\n",
    "    for i in range(x.shape[0]):\n",
    "        x_forw = np.array(x) # Make a copy of x\n",
    "        x_forw[i] += h   # Increment x_forw[i] by h\n",
    "        delta_f[i] = (f(x_forw) - f(x)) / h # Calculate the forward difference\n",
    "\n",
    "    return delta_f\n",
    "\n",
    "\n",
    "# Central difference:\n",
    "def central_diff(f, x, h=1E-08):\n",
    "    # f: function\n",
    "    # x: vector of variables\n",
    "    # h: step size\n",
    "\n",
    "    delta_f = np.zeros(x.shape) # Initialize delta_f\n",
    "    for i in range(x.shape[0]):\n",
    "        x_forw = np.array(x) # Make a copy of x\n",
    "        x_back = np.array(x) # Make a copy of x\n",
    "        x_forw[i] += h   # Increment x_forw[i] by h\n",
    "        x_back[i] -= h   # Decrement x_back[i] by h\n",
    "        delta_f = (f(x_forw) - f(x_back)) / (2*h) # Calculate the central difference\n",
    "\n",
    "    return delta_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting constrained optimization problem to unconstrained optimization problem:\n",
    "# Using penalty method:\n",
    "\n",
    "def constrained_to_unconstrained(f, g, h, x, p=100):\n",
    "    # f: objective function\n",
    "    # g: inequality constraint\n",
    "    # h: equality constraint\n",
    "    # x: vector of variables\n",
    "    # p: penalty parameter\n",
    "\n",
    "    # Objective function\n",
    "    def f_unconstrained(x):\n",
    "        return f(x) + p*(g(x)**2 + h(x)**2)\n",
    "\n",
    "    return f_unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unconstrained optimization:\n",
    "# 1st order methods:\n",
    "\n",
    "# Steepest descent method:\n",
    "def steepest_descent(f, x, delta_f, delta_f_tol=1E-08, delta_x_tol=1E-08, max_iter=1000, full_output=False):\n",
    "    # f: function\n",
    "    # x: vector of variables\n",
    "    # delta_f: method for calculating the gradient\n",
    "\n",
    "    # exit flags:\n",
    "    # 0: maximum number of iterations reached\n",
    "    # 1: gradient is within tolerance\n",
    "    # 2: design variable update is within tolerance\n",
    "\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "    while not convergence:\n",
    "        s = -delta_f(f, x) # Calculate the search direction\n",
    "        s = s / np.sqrt(np.dot(s, s)) # Normalize the search direction\n",
    "        alpha = opt.fminbound(lambda alpha: f(x + alpha*s), 0, 1) # Calculate the step size\n",
    "        x_new = x + alpha*s # Calculate the new x\n",
    "        iter += 1 # Increment the iteration counter\n",
    "        \n",
    "        # Check for convergence:\n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.sqrt(np.dot(delta_f(f, x_new), delta_f(f, x_new))) <= delta_f_tol:    # Check if gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.sqrt(np.dot(x_new - x, x_new - x)) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        x = x_new # Update x\n",
    "\n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    else:\n",
    "        return x_new\n",
    "\n",
    "\n",
    "# Conjugate gradient method:\n",
    "def conjugate_gradient(f, x, delta_f, delta_f_tol=1E-08, delta_x_tol=1E-08, max_iter=1000, full_output=False):\n",
    "    # f: function\n",
    "    # x: vector of variables\n",
    "    # delta_f: method for calculating the gradient\n",
    "    \n",
    "    # exit flags:\n",
    "    # 0: maximum number of iterations reached\n",
    "    # 1: gradient is within tolerance\n",
    "    # 2: design variable update is within tolerance\n",
    "\n",
    "    s = -delta_f(f, x) # Calculate the initial search direction\n",
    "    s = s / np.sqrt(np.dot(s, s)) # Normalize the initial search direction\n",
    "    alpha = opt.fminbound(lambda alpha: f(x + alpha*s), 0, 1) # Calculate the initial step size\n",
    "    x_new = x + alpha*s # Calculate the initial x update\n",
    "\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "    while not convergence:\n",
    "        if iter % x.shape[0] == 0:  # Check if the iteration counter is a multiple of the number of design variables\n",
    "            s = -delta_f(f, x_new) # Calculate the search direction\n",
    "        else:\n",
    "            s = -delta_f(f, x_new) + np.sqrt(np.dot(delta_f(f, x_new), delta_f(f, x_new)) / np.dot(delta_f(f, x), delta_f(f, x))) * s # Calculate the search direction\n",
    "\n",
    "        s = s / np.sqrt(np.dot(s, s)) # Normalize the search direction\n",
    "        alpha = opt.fminbound(lambda alpha: f(x_new + alpha*s), 0, 1) # Calculate the step size\n",
    "        x = x_new # Update x\n",
    "        x_new = x + alpha*s # Calculate the new x\n",
    "        iter += 1 # Increment the iteration counter\n",
    "\n",
    "        # Check for convergence:\n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.sqrt(np.dot(delta_f(f, x_new), delta_f(f, x_new))) <= delta_f_tol:    # Check if gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.sqrt(np.dot(x_new - x, x_new - x)) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        \n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    else:\n",
    "        return x_new\n",
    "\n",
    "# Quasi-Newton methods:\n",
    "# BFGS update rule:\n",
    "def BFGS_update(B, delta_x, delta_delta):\n",
    "    # B: Hessian approximation\n",
    "    # delta_x: design variable update\n",
    "    # delta_delta: gradient update\n",
    "\n",
    "    return (1 + delta_delta.T@B@delta_delta / (delta_delta.T@delta_x)) * (delta_x@delta_x.T) / (delta_x.T@delta_delta) - \\\n",
    "        (delta_x@(delta_delta.T@B) + (delta_delta.T@B).T@delta_x.T) / (delta_x.T@delta_delta)\n",
    "\n",
    "# DFP update rule:\n",
    "def DFP_update(B, delta_x, delta_delta):\n",
    "    # B: Hessian approximation\n",
    "    # delta_x: design variable update\n",
    "    # delta_delta: gradient update\n",
    "\n",
    "    return delta_x@delta_x.T / (delta_x.T@delta_delta) - \\\n",
    "        (B@delta_delta)@(B@delta_delta).T / (delta_delta.T@B@delta_delta)\n",
    "\n",
    "# Quasi-Newton method:\n",
    "def quasi_newton(f, x, delta_f, delta_f_tol=1E-08, delta_x_tol=1E-08, max_iter=1000, full_output=False, update_rule=BFGS_update):\n",
    "    # f: function\n",
    "    # x: vector of variables\n",
    "    # delta_f: method for calculating the gradient\n",
    "    # update_rule: method for updating the Hessian approximation\n",
    "\n",
    "    # exit flags:\n",
    "    # 0: maximum number of iterations reached\n",
    "    # 1: gradient is within tolerance\n",
    "    # 2: design variable update is within tolerance\n",
    "\n",
    "    B = np.eye(x.shape[0]) # Initialize the Hessian approximation\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "\n",
    "    while not convergence:\n",
    "        s = -B@delta_f(f, x)   # Calculate the search direction\n",
    "        s = s / np.sqrt(np.dot(s, s)) # Normalize the search direction\n",
    "        alpha = opt.fminbound(lambda alpha: f(x + alpha*s), 0, 1) # Calculate the step size\n",
    "        x_new = x + alpha*s # Calculate the new x\n",
    "        iter += 1 # Increment the iteration counter\n",
    "\n",
    "        # Check for convergence:\n",
    "        if iter >= max_iter:\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.sqrt(np.dot(delta_f(f, x_new), delta_f(f, x_new))) <= delta_f_tol:    # Check if gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.sqrt(np.dot(x_new - x, x_new - x)) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "\n",
    "        B = update_rule(B, x_new - x, delta_f(f, x_new) - delta_f(f, x)) # Update the Hessian approximation\n",
    "        x = x_new # Update x\n",
    "\n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    else:\n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained optimization methods:\n",
    "# Function to calculate the lagrangian of function:\n",
    "def calc_lagrangian(f, con, x, lambda_, mu):\n",
    "    # f: function to be minimized\n",
    "    # x: vector of variables\n",
    "    # lambda_: vector of lagrange multipliers for inequality constraints\n",
    "    # mu: vector of lagrange multipliers for equality constraints\n",
    "    # g: vector of inequality constraints\n",
    "    # h: vector of equality constraints\n",
    "\n",
    "    \n",
    "    g, h = con(x)   # Calculate the inequality and equality constraints\n",
    "    \n",
    "    return f(x) + mu@g + lambda_@h  # Calculate the lagrangian\n",
    "\n",
    "\n",
    "# Augmented Lagrangian method:\n",
    "# Function to calculate the augmented Lagrangian function:\n",
    "def calc_augmented_lagrangian(f, con, x, lambda_, mu, rho):\n",
    "    # f: objective function\n",
    "    # con: constraint function\n",
    "    # x: vector of variables\n",
    "    # lambda_: Lagrange inequality multiplier\n",
    "    # mu: Lagrange equality multiplier\n",
    "    # rho: penalty parameter\n",
    "\n",
    "    g, h = con(x) # Calculate the inequality and equality constraints\n",
    "    \n",
    "    return calc_lagrangian(f, con, x, lambda_, mu) + rho*(np.dot(g, g) + np.dot(h, h)) # Calculate the augmented Lagrangian function\n",
    "\n",
    "def augmented_lagrangian(f, con, x, delta_f, rho=1, delta_f_tol=1E-08, delta_x_tol=1E-08, max_iter=1000, full_output=False):\n",
    "    # f: function\n",
    "    # x: vector of variables\n",
    "    # con: constraint function\n",
    "    # delta_f: method for calculating the gradient\n",
    "    \n",
    "    # exit flags:\n",
    "    # 0: maximum number of iterations reached\n",
    "    # 1: lagrangian gradient is within tolerance\n",
    "    # 2: design variable update is within tolerance\n",
    "    \n",
    "    g, h = con(x) # Calculate the initial constraint values\n",
    "    mu0 = np.zeros(g.shape[0]) # Initialize the inequality Lagrangian multipliers\n",
    "    lambda_0 = np.zeros(h.shape[0]) # Initialize the equality Lagrangian multipliers\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "\n",
    "    while not convergence:\n",
    "        x_new = quasi_newton(lambda x: calc_augmented_lagrangian(f, con, x, lambda_0, mu0, rho), \\\n",
    "            x, delta_f, delta_f_tol, delta_x_tol, max_iter, full_output=False) # Optimize the augmented Lagrangian function\n",
    "        g, h = con(x_new) # Calculate the constraint values\n",
    "        mu_new = mu0 + rho*g # Calculate the new inequality Lagrangian multipliers\n",
    "        # Check for negative inequality multipliers:\n",
    "        for i in range(len(mu_new)):\n",
    "            if mu_new[i] < 0:\n",
    "                mu_new[i] = 0\n",
    "        lambda_new = lambda_0 + rho*h # Calculate the new equality Lagrangian multipliers\n",
    "        delta_L_new = delta_f(lambda x: calc_lagrangian(f, con, x, lambda_new, mu_new), x_new) # Calculate the new Lagrangian gradient\n",
    "        iter += 1 # Increment the iteration counter\n",
    "        \n",
    "        # Check for convergence:\n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.sqrt(np.dot(delta_L_new, delta_L_new)) <= delta_f_tol: # Check if lagrangian gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.sqrt(np.dot(x_new - x, x_new - x)) <= delta_x_tol: # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        \n",
    "        x = x_new # Update x\n",
    "        mu0 = mu_new # Update the Lagrangian multipliers\n",
    "        lambda_0 = lambda_new # Update the Lagrangian multipliers\n",
    "\n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    else:\n",
    "        return x_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('engineering_optimization')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2f828b6b8c3a80642846a1683196e611a19f96e87bf6e9047e3ef43d9a50772"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
