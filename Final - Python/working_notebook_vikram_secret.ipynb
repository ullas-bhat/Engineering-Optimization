{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries for optimization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(A, b):\n",
    "    '''\n",
    "    Solves the least squares problem Ax = b\n",
    "    Args:\n",
    "        A: co-efficients matrix\n",
    "        b: constants vector\n",
    "    Returns:\n",
    "        x: solution vector\n",
    "    '''\n",
    "\n",
    "    return np.linalg.inv(A.T @ A) @ A.T @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite difference methods:\n",
    "def forward_diff(f, x, h=1E-08):\n",
    "    '''\n",
    "    Solves the forward difference problem\n",
    "    Args:\n",
    "        f: function\n",
    "        x: vector of variables\n",
    "        h: step size\n",
    "    Returns:\n",
    "        delta_f: solution vector\n",
    "    '''\n",
    "\n",
    "    delta_f = np.zeros(x.shape) # Initialize delta_f\n",
    "    for i in range(x.shape[0]):\n",
    "        x_forw = np.array(x) # Make a copy of x\n",
    "        x_forw[i] += h   # Increment x_forw[i] by h\n",
    "        delta_f[i] = (f(x_forw) - f(x)) / h # Calculate the forward difference\n",
    "\n",
    "    return delta_f\n",
    "\n",
    "\n",
    "def central_diff(f, x, h=1E-08):\n",
    "    '''\n",
    "    Solves the central difference problem\n",
    "    Args:\n",
    "        f: function\n",
    "        x: vector of variables\n",
    "        h: step size\n",
    "    Returns:\n",
    "        delta_f: solution vector\n",
    "    '''\n",
    "\n",
    "    delta_f = np.zeros(x.shape) # Initialize delta_f\n",
    "    for i in range(x.shape[0]):\n",
    "        x_forw = np.array(x) # Make a copy of x\n",
    "        x_back = np.array(x) # Make a copy of x\n",
    "        x_forw[i] += h   # Increment x_forw[i] by h\n",
    "        x_back[i] -= h   # Decrement x_back[i] by h\n",
    "        delta_f [i]= (f(x_forw) - f(x_back)) / (2*h) # Calculate the central difference\n",
    "\n",
    "    return delta_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting constrained optimization problem to unconstrained optimization problem:\n",
    "# Using penalty method:\n",
    "def constrained_to_unconstrained(f, con, x, p=100):\n",
    "    '''\n",
    "    Converts a constrained optimization problem to an unconstrained optimization problem\n",
    "    Args:\n",
    "        f: function\n",
    "        con: constraint function\n",
    "        x: vector of variables\n",
    "        p: penalty parameter\n",
    "    Returns:\n",
    "        f_uncon: unconstrained function\n",
    "    '''\n",
    "    \n",
    "    # Objective function\n",
    "    def f_unconstrained(x):\n",
    "        g, h = con(x) # Calculate the constraint function\n",
    "        con_sum = 0 # Initialize the constraint sum\n",
    "        for i in range(g.shape[0]):\n",
    "            con_sum += max(0, g[i])**2 # Add the constraint sum\n",
    "        for i in range(h.shape[0]):\n",
    "            con_sum += h[i]**2  # Add the constraint sum\n",
    "        return f(x) + p*con_sum # Return the unconstrained objective function\n",
    "        \n",
    "    return f_unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unconstrained optimization:\n",
    "# 1st order methods:\n",
    "\n",
    "def steepest_descent(f, x, delta_f, grad_tol=1E-05, delta_f_tol=1E-05, delta_x_tol=1E-05, max_iter=100, full_output=False):\n",
    "    '''\n",
    "    Optimizes the unconstrained problem using the steepest descent method\n",
    "    Args:\n",
    "        f: objective function\n",
    "        x: initial value of vector of variables\n",
    "        delta_f: gradient of the objective function\n",
    "        grad_tol: gradient tolerance\n",
    "        delta_f_tol: objective function update tolerance\n",
    "        delta_x_tol: design variable update tolerance\n",
    "        max_iter: maximum number of iterations\n",
    "        full_output: whether to return the full output or not\n",
    "    Returns:\n",
    "        x: solution vector\n",
    "        f_val: objective function value\n",
    "        exit_flag: exit flag\n",
    "        iter: number of iterations\n",
    "    '''\n",
    "\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "    while not convergence:\n",
    "        s = -delta_f(f, x) # Calculate the search direction\n",
    "        s = s / np.linalg.norm(s) # Normalize the search direction\n",
    "        alpha = opt.fminbound(lambda alpha: f(x + alpha*s), 0, 1) # Calculate the step size\n",
    "        x_new = x + alpha*s # Calculate the new x\n",
    "        iter += 1 # Increment the iteration counter\n",
    "        \n",
    "        # Check for convergence:\n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.linalg.norm(delta_f(f, x_new)) <= grad_tol:    # Check if gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.linalg.norm(x_new - x) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        elif np.linalg.norm(f(x_new) - f(x)) <= delta_f_tol:    # Check if objective function update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 3\n",
    "            \n",
    "        x = x_new # Update x\n",
    "\n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    else:\n",
    "        return x_new\n",
    "\n",
    "\n",
    "def conjugate_gradient(f, x, delta_f, grad_tol=1E-05, delta_f_tol=1E-05, delta_x_tol=1E-05, max_iter=100, full_output=False):\n",
    "    '''\n",
    "    Optimizes the unconstrained problem using the conjugate gradient method\n",
    "    Args:\n",
    "        f: objective function\n",
    "        x: initial value of vector of variables\n",
    "        delta_f: gradient of the objective function\n",
    "        grad_tol: gradient tolerance\n",
    "        delta_f_tol: objective function update tolerance\n",
    "        delta_x_tol: design variable update tolerance\n",
    "        max_iter: maximum number of iterations\n",
    "        full_output: whether to return the full output or not\n",
    "    Returns:\n",
    "        x: solution vector\n",
    "        f_val: objective function value\n",
    "        exit_flag: exit flag\n",
    "        iter: number of iterations\n",
    "    '''\n",
    "\n",
    "    s = -delta_f(f, x) # Calculate the initial search direction\n",
    "    s = s / np.linalg.norm(s) # Normalize the initial search direction\n",
    "    alpha = opt.fminbound(lambda alpha: f(x + alpha*s), 0, 1) # Calculate the initial step size\n",
    "    x_new = x + alpha*s # Calculate the initial x update\n",
    "\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "    while not convergence:\n",
    "        if iter % x.shape[0] == 0:  # Check if the iteration counter is a multiple of the number of design variables\n",
    "            s = -delta_f(f, x_new) # Calculate the search direction\n",
    "        else:\n",
    "            s = -delta_f(f, x_new) + (np.linalg.norm(delta_f(f, x_new))**2 / np.linalg.norm(delta_f(f, x))**2) * s # Calculate the search direction\n",
    "\n",
    "        s = s / np.linalg.norm(s) # Normalize the search direction\n",
    "        alpha = opt.fminbound(lambda alpha: f(x_new + alpha*s), 0, 1) # Calculate the step size\n",
    "        x = x_new # Update x\n",
    "        x_new = x + alpha*s # Calculate the new x\n",
    "        iter += 1 # Increment the iteration counter\n",
    "\n",
    "        # Check for convergence:\n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.linalg.norm(delta_f(f, x_new)) <= grad_tol:    # Check if gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.linalg.norm(x_new - x) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        elif np.linalg.norm(f(x_new) - f(x)) <= delta_f_tol:    # Check if objective function update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 3\n",
    "        \n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    else:\n",
    "        return x_new\n",
    "\n",
    "\n",
    "# Hessian update rules:\n",
    "def BFGS_update(B, delta_x, delta_delta):\n",
    "    '''\n",
    "    Calculates the BFGS update matrix\n",
    "    Args:\n",
    "        B: initial Hessian estimate\n",
    "        delta_x: design variable update\n",
    "        delta_delta: objective function update\n",
    "    Returns:\n",
    "        deta_B: update to the Hessian estimate\n",
    "    '''\n",
    "\n",
    "    return (1 + delta_delta.T@B@delta_delta / (delta_delta.T@delta_x)) * (delta_x@delta_x.T) / (delta_x.T@delta_delta) - \\\n",
    "        (delta_x@(delta_delta.T@B) + (delta_delta.T@B).T@delta_x.T) / (delta_x.T@delta_delta)\n",
    "\n",
    "def DFP_update(B, delta_x, delta_delta):\n",
    "    '''\n",
    "    Calculates the DFP update matrix\n",
    "    Args:\n",
    "        B: initial Hessian estimate\n",
    "        delta_x: design variable update\n",
    "        delta_delta: objective function update\n",
    "    Returns:\n",
    "        deta_B: update to the Hessian estimate\n",
    "    '''\n",
    "\n",
    "    return delta_x@delta_x.T / (delta_x.T@delta_delta) - \\\n",
    "        (B@delta_delta)@(B@delta_delta).T / (delta_delta.T@B@delta_delta)\n",
    "\n",
    "\n",
    "# Quasi-Newton method:\n",
    "def quasi_newton(f, x, delta_f, grad_tol=1E-05, delta_f_tol=1E-05, delta_x_tol=1E-05, max_iter=100, full_output=False, update_rule=BFGS_update):\n",
    "    '''\n",
    "    Optimizes the unconstrained problem using the quasi-Newton method\n",
    "    Args:\n",
    "        f: objective function\n",
    "        x: initial value of vector of variables\n",
    "        delta_f: gradient of the objective function\n",
    "        grad_tol: gradient tolerance\n",
    "        delta_f_tol: objective function update tolerance\n",
    "        delta_x_tol: design variable update tolerance\n",
    "        max_iter: maximum number of iterations\n",
    "        full_output: whether to return the full output or not\n",
    "        update_rule: update rule for the Hessian estimate\n",
    "    Returns:\n",
    "        x: solution vector\n",
    "        f_val: objective function value\n",
    "        exit_flag: exit flag\n",
    "        iter: number of iterations\n",
    "    '''\n",
    "\n",
    "    B = np.eye(x.shape[0]) # Initialize the Hessian approximation\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "\n",
    "    while not convergence:\n",
    "        s = -B@delta_f(f, x)   # Calculate the search direction\n",
    "        s = s / np.linalg.norm(s) # Normalize the search direction\n",
    "        alpha = opt.fminbound(lambda alpha: f(x + alpha*s), 0, 1) # Calculate the step size\n",
    "        x_new = x + alpha*s # Calculate the new x\n",
    "        iter += 1 # Increment the iteration counter\n",
    "\n",
    "         # Check for convergence:\n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.linalg.norm(delta_f(f, x_new)) <= grad_tol:    # Check if gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.linalg.norm(x_new - x) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        elif np.linalg.norm(f(x_new) - f(x)) <= delta_f_tol:    # Check if objective function update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 3\n",
    "\n",
    "        B = B + update_rule(B, x_new - x, delta_f(f, x_new) - delta_f(f, x)) # Update the Hessian approximation\n",
    "        x = x_new # Update x\n",
    "\n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    else:\n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained optimization methods:\n",
    "\n",
    "def calc_lagrangian(f, con, x, lambda_, mu):\n",
    "    '''\n",
    "    Calculates the lagrangian of the function\n",
    "    Args:\n",
    "        f: objective function\n",
    "        con: constraint function\n",
    "        x: vector of variables\n",
    "        lambda_: Lagrange equality multipliers\n",
    "        mu: Lagrange inequality multipliers\n",
    "    Returns:\n",
    "        lagrangian: lagrangian of the function\n",
    "    '''\n",
    "    g, h = con(x)   # Calculate the inequality and equality constraints\n",
    "    \n",
    "    return f(x) + lambda_@h + mu@g   # Calculate the lagrangian\n",
    "\n",
    "\n",
    "# Augmented Lagrangian method:\n",
    "def calc_augmented_lagrangian(f, con, x, lambda_, mu, rho):\n",
    "    '''\n",
    "    Calculates the augmented Lagrangian of the function\n",
    "    Args:\n",
    "        f: objective function\n",
    "        con: constraint function\n",
    "        x: vector of variables\n",
    "        lambda_: Lagrange equality multipliers\n",
    "        mu: Lagrange inequality multipliers\n",
    "        rho: augmented Lagrangian penalty parameter\n",
    "    Returns:\n",
    "        augmented_lagrangian: augmented Lagrangian of the function\n",
    "    '''\n",
    "\n",
    "    g, h = con(x) # Calculate the inequality and equality constraints\n",
    "    con_sum = 0\n",
    "    for i in range(g.shape[0]):\n",
    "        con_sum += max(0, g[i])**2\n",
    "    for i in range(h.shape[0]):\n",
    "        con_sum += h[i]**2\n",
    "    \n",
    "    return calc_lagrangian(f, con, x, lambda_, mu) +  rho * con_sum # Calculate the augmented Lagrangian function\n",
    "\n",
    "\n",
    "def augmented_lagrangian(f, con, x, delta_f, rho=1, grad_tol=1E-05, delta_f_tol=1E-05, delta_x_tol=1E-05, max_iter=100, full_output=False):\n",
    "    '''\n",
    "    Optimizes the constrained problem using the augmented Lagrangian method\n",
    "    Args:\n",
    "        f: objective function\n",
    "        con: constraint function\n",
    "        x: initial value of vector of variables\n",
    "        delta_f: gradient of the objective function\n",
    "        rho: augmented Lagrangian penalty parameter\n",
    "        grad_tol: gradient tolerance\n",
    "        delta_f_tol: objective function update tolerance\n",
    "        delta_x_tol: design variable update tolerance\n",
    "        max_iter: maximum number of iterations\n",
    "        full_output: whether to return the full output or not\n",
    "    Returns:\n",
    "        x: solution vector\n",
    "        f_val: objective function value\n",
    "        exit_flag: exit flag\n",
    "        iter: number of iterations\n",
    "    '''\n",
    "    \n",
    "    g, h = con(x) # Calculate the initial constraint values\n",
    "    mu0 = np.zeros(g.shape[0]).T # Initialize the inequality Lagrangian multipliers\n",
    "    lambda_0 = np.zeros(h.shape[0]).T # Initialize the equality Lagrangian multipliers\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "\n",
    "    while not convergence:\n",
    "        x_new = quasi_newton(lambda x: calc_augmented_lagrangian(f, con, x, lambda_0, mu0, rho), \\\n",
    "            x, delta_f, delta_f_tol, delta_x_tol, max_iter=100, full_output=False) # Optimize the augmented Lagrangian function\n",
    "        g, h = con(x_new) # Calculate the constraint values\n",
    "        mu_new = mu0 + rho*g # Calculate the new inequality Lagrangian multipliers\n",
    "        # Check for negative inequality multipliers:\n",
    "        for i in range(len(mu_new)):\n",
    "            if mu_new[i] < 0:\n",
    "                mu_new[i] = 0\n",
    "        lambda_new = lambda_0 + rho*h # Calculate the new equality Lagrangian multipliers\n",
    "        delta_L_new = delta_f(lambda x: calc_lagrangian(f, con, x, lambda_new, mu_new), x_new) # Calculate the new Lagrangian gradient\n",
    "        iter += 1 # Increment the iteration counter\n",
    "        \n",
    "        # Check for convergence:\n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.linalg.norm(delta_f(f, x_new)) <= grad_tol:    # Check if gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.linalg.norm(x_new - x) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        elif np.linalg.norm(f(x_new) - f(x)) <= delta_f_tol:    # Check if objective function update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 3\n",
    "        \n",
    "        x = x_new # Update x\n",
    "        mu0 = mu_new # Update the Lagrangian multipliers\n",
    "        lambda_0 = lambda_new # Update the Lagrangian multipliers\n",
    "\n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test objective function\n",
    "def f(x):\n",
    "    return ( ( 1-x[0] )**2 + 100* ( x[1]-x[0]**2) **2)\n",
    "\n",
    "# Test constraint function\n",
    "def con(x):\n",
    "    g = np.array([])\n",
    "    h = np.array([])\n",
    "    return g, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nelder Mead Simplex Optimiser (f= Objective Function, x0= Scaled initial vector)\n",
    "#http://www.scholarpedia.org/article/Nelder-Mead_algorithm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Simplexsolver(func,x0,max_iter=10000,pert=0.25):\n",
    "    iter=0\n",
    "    function_array=np.zeros( ( x0.shape[0]+1,x0.shape[0]+1) )  \n",
    "    i=0\n",
    "    function_array[0,0]=func(x0)\n",
    "    function_array[1:,0]=x0[:,0]\n",
    "\n",
    "    print(func(x0))\n",
    "    print(function_array.shape)\n",
    "    while i < (len(function_array[1])-1):\n",
    "      xtemp=x0\n",
    "      xtemp[i]+=pert\n",
    "      function_array[0,i+1]=func(xtemp)\n",
    "      function_array[1:,i+1]=xtemp[:,0]\n",
    "      i +=1\n",
    "    \n",
    " #Parameters for simplex reflect,extend, shrink,etc(using commonly used values)\n",
    "\n",
    "    alpha_simp=1;\n",
    "    beta_simp=0.5;\n",
    "    gamma_simp=2;\n",
    "    delta_simp=0.5;\n",
    "    \n",
    "    delta_x_tol=0.0001\n",
    "    delta_f_tol=0.0001\n",
    "    previous_objective= 10000\n",
    "\n",
    "    convergence=False\n",
    "    while convergence==False:\n",
    "        iter +=1   \n",
    "        shrink=0\n",
    "        \n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.linalg.norm(function_array[1:,-1] - function_array[1:,0]) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        elif np.linalg.norm(function_array[0,0] - function_array[0,-1]) <= delta_f_tol:    # Check if objective function update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 3\n",
    "\n",
    "       # print(function_array) \n",
    "       # print(function_array[0, :].argsort())\n",
    "        function_array = function_array[:,function_array[0,:].argsort()]\n",
    "        \n",
    "        #print(function_array)\n",
    "        previous_objective==function_array[0,-1]\n",
    "        # Finding Centroid     \n",
    "        c=np.zeros((x0.shape[0]))\n",
    "        i=0\n",
    "        while i < len(function_array[1]-1):\n",
    "           c = c+function_array[1:,i]\n",
    "           #print(c)\n",
    "           i +=1\n",
    "      \n",
    "        c = c/(len(function_array[1])-1)\n",
    "        #print(len(function_array[1]))\n",
    "        #print(c)\n",
    "        # Compute New Reflection Point \n",
    "        xr= c + alpha_simp * (c - function_array[1:,-1])\n",
    "        print(c)\n",
    "        print(xr)\n",
    "        reflect_funcval = func(xr)\n",
    "        print(reflect_funcval)\n",
    "        print(function_array)\n",
    "\n",
    "        if reflect_funcval < function_array[0,-2] and reflect_funcval >= function_array[0,0]:\n",
    "          function_array[0,-1]= reflect_funcval\n",
    "          function_array[1:,-1]= xr[:]\n",
    "          print(\"reflected\")\n",
    "          continue\n",
    "         \n",
    "\n",
    "        if reflect_funcval  < function_array[0,0]:\n",
    "             #Expand Simplex\n",
    "             xe=c+gamma_si\n",
    "             mp*(xr-c)\n",
    "             expansion_funcval=func(xe)\n",
    "             if expansion_funcval< reflect_funcval :\n",
    "                function_array[0,-1]= expansion_funcval\n",
    "                function_array[1:,-1]= xe[:]\n",
    "                print(\"expanded\")\n",
    "                continue\n",
    "\n",
    "             else:\n",
    "                 function_array[0,-1]= reflect_funcval \n",
    "                 function_array[1:,-1]= xr[:] \n",
    "                 print(\"reflected\")\n",
    "                 continue\n",
    "\n",
    "        if reflect_funcval  >= function_array[0,-2] :\n",
    "                #Contract Simplex\n",
    "                if reflect_funcval < function_array[0,-1]  :\n",
    "                  xc= c + beta_simp * ( xr - c )\n",
    "                  contract_funcval= func(xc)\n",
    "                  if contract_funcval < reflect_funcval:\n",
    "                     function_array[0,-1]= contract_funcval \n",
    "                     function_array[1:,-1]= xc[:]\n",
    "                     print(\"Contracted\")\n",
    "                     continue\n",
    "                  else:\n",
    "                     shrink=1\n",
    "\n",
    "                elif reflect_funcval>=function_array[0,-1] :   \n",
    "                   xc= c + beta_simp * ( function_array[1:,-1] - c )\n",
    "                   contract_funcval= func(xc)\n",
    "                   if(contract_funcval <function_array[0,-1]):\n",
    "                     function_array[0,-1]= contract_funcval \n",
    "                     function_array[1:,-1]= xc[:]\n",
    "                     print(\"Contracted\")\n",
    "                     continue\n",
    "                   else:\n",
    "                    shrink=1  \n",
    "        if shrink==1:     \n",
    "            print(\"Shrunk\")\n",
    "            j=0\n",
    "            print(function_array.shape[0])\n",
    "            #Shrink Transformation, shrink each lattice towards the lowest value.\n",
    "            while(j<function_array.shape[0]-1):\n",
    "               function_array[1:,j+1]=function_array[1:,0] + delta_simp * (function_array[1:,j+1]-function_array[1:,0])\n",
    "               function_array[0,j+1]=func(function_array[1:,j])\n",
    "               j+=1\n",
    "\n",
    "    return iter,function_array,function_array[0,-1],exit_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6]\n",
      "(3, 3)\n",
      "[1.45  1.325]\n",
      "[1.85 1.85]\n",
      "247.99812500000027\n",
      "[[0.278125 2.6      9.153125]\n",
      " [1.05     0.8      1.05    ]\n",
      " [1.05     0.8      0.8     ]]\n",
      "Shrunk\n",
      "3\n",
      "[1.5125 1.45  ]\n",
      "[1.975 1.975]\n",
      "371.7537890625003\n",
      "[[0.278125   0.278125   0.48691406]\n",
      " [1.05       0.925      1.05      ]\n",
      " [1.05       0.925      0.925     ]]\n",
      "Shrunk\n",
      "3\n",
      "[1.54375 1.5125 ]\n",
      "[2.1    2.0375]\n",
      "564.0856250000007\n",
      "[[0.01539307 0.278125   0.278125  ]\n",
      " [1.05       1.05       0.9875    ]\n",
      " [0.9875     1.05       0.9875    ]]\n",
      "Shrunk\n",
      "3\n",
      "[1.559375 1.496875]\n",
      "[2.06875 1.975  ]\n",
      "532.3186793518074\n",
      "[[0.01539307 0.70390625 1.325     ]\n",
      " [1.05       1.01875    1.05      ]\n",
      " [0.9875     0.9875     1.01875   ]]\n",
      "Shrunk\n",
      "3\n",
      "[1.5671875 1.4890625]\n",
      "[2.1      1.990625]\n",
      "586.5475390625007\n",
      "[[0.01539307 0.68067918 1.325     ]\n",
      " [1.05       1.05       1.034375  ]\n",
      " [0.9875     1.003125   0.9875    ]]\n",
      "Shrunk\n",
      "3\n",
      "[1.57109375 1.48515625]\n",
      "[2.0921875 1.975    ]\n",
      "578.272676001192\n",
      "[[0.01539307 1.15141602 1.325     ]\n",
      " [1.05       1.0421875  1.05      ]\n",
      " [0.9875     0.9875     0.9953125 ]]\n",
      "Shrunk\n",
      "3\n",
      "[1.57304688 1.48320313]\n",
      "[2.1        1.97890625]\n",
      "592.2316821289069\n",
      "[[0.01539307 1.14300783 1.325     ]\n",
      " [1.05       1.05       1.04609375]\n",
      " [0.9875     0.99140625 0.9875    ]]\n",
      "Shrunk\n",
      "3\n",
      "[1.57402344 1.48222656]\n",
      "[2.09804688 1.975     ]\n",
      "590.1418656912106\n",
      "[[0.01539307 1.28045959 1.325     ]\n",
      " [1.05       1.04804688 1.05      ]\n",
      " [0.9875     0.9875     0.98945313]]\n",
      "Shrunk\n",
      "3\n",
      "[1.57451172 1.48173828]\n",
      "[2.1        1.97597656]\n",
      "593.6570094299324\n",
      "[[0.01539307 1.27817744 1.325     ]\n",
      " [1.05       1.05       1.04902344]\n",
      " [0.9875     0.98847656 0.9875    ]]\n",
      "Shrunk\n",
      "3\n",
      "[1.57475586 1.48149414]\n",
      "[2.09951172 1.975     ]\n",
      "593.1332321333578\n",
      "[[0.01539307 1.31379337 1.325     ]\n",
      " [1.05       1.04951172 1.05      ]\n",
      " [0.9875     0.9875     0.98798828]]\n",
      "Shrunk\n",
      "3\n",
      "[1.57487793 1.48137207]\n",
      "[2.1        1.97524414]\n",
      "594.0136094760902\n",
      "[[0.01539307 1.3132113  1.325     ]\n",
      " [1.05       1.05       1.04975586]\n",
      " [0.9875     0.98774414 0.9875    ]]\n",
      "Shrunk\n",
      "3\n",
      "[1.57493896 1.48131104]\n",
      "[2.09987793 1.975     ]\n",
      "593.882582384013\n",
      "[[0.01539307 1.32219387 1.325     ]\n",
      " [1.05       1.04987793 1.05      ]\n",
      " [0.9875     0.9875     0.98762207]]\n",
      "Shrunk\n",
      "3\n",
      "[1.57496948 1.48128052]\n",
      "[2.1        1.97506104]\n",
      "594.102776251436\n",
      "[[0.01539307 1.32204763 1.325     ]\n",
      " [1.05       1.05       1.04993896]\n",
      " [0.9875     0.98756104 0.9875    ]]\n",
      "Shrunk\n",
      "3\n",
      "(13, array([[0.01539307, 1.325     , 1.32429819],\n",
      "       [1.05      , 1.05      , 1.04996948],\n",
      "       [0.9875    , 0.98753052, 0.9875    ]]), 1.3242981888353822, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " final_simplex: (array([[1.00001544, 1.00003065],\n",
       "       [0.99999067, 0.9999826 ],\n",
       "       [0.99997926, 0.99995717]]), array([2.43816670e-10, 2.46521910e-10, 6.16283605e-10]))\n",
       "           fun: 2.438166698817486e-10\n",
       "       message: 'Optimization terminated successfully.'\n",
       "          nfev: 69\n",
       "           nit: 35\n",
       "        status: 0\n",
       "       success: True\n",
       "             x: array([1.00001544, 1.00003065])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0.8, 0.8 ]], dtype=float).T # Initialize the design variable\n",
    "\n",
    "f_unc = constrained_to_unconstrained(f, con, x) # Convert the constrained function to an unconstrained function\n",
    "\n",
    "print(Simplexsolver(f_unc,x))\n",
    "\n",
    "opt.minimize(f_unc, x, args=(), method='Nelder-Mead')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bf06b8021c62b15361c0f1ff6f7b4a98d4739984d2745f50473cee6c668c345"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
